{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IECore\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import numpy as np\n",
    "ie = IECore()\n",
    "video_path = r'data_videos\\istockphoto-465105760-640_adpp_is.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head_pose_angles(image):\n",
    "    head_pose_xml = r'models\\head-pose-estimation-adas-0001\\head-pose-estimation-adas-0001.xml'\n",
    "    head_pose_bin = r'models\\head-pose-estimation-adas-0001\\head-pose-estimation-adas-0001.bin'\n",
    "    net_head_pose = ie.read_network(model = head_pose_xml,weights = head_pose_bin)\n",
    "    ext_head_pose = ie.load_network(network = net_head_pose,device_name='CPU')\n",
    "    input_name = next(iter(net_head_pose.input_info))\n",
    "    input_info = net_head_pose.input_info[input_name]\n",
    "\n",
    "    head_resized = cv2.resize(image,(60,60))\n",
    "    preprocessed_image = head_resized.transpose((2, 0, 1)) \n",
    "    output = ext_head_pose.infer(inputs={input_name: preprocessed_image})\n",
    "    yaw = output['angle_y_fc'].flatten()[0]  # Extract the yaw angle value\n",
    "    pitch = output['angle_p_fc'].flatten()[0]  # Extract the pitch angle value\n",
    "    roll = output['angle_r_fc'].flatten()[0]  # Extract the roll angle valu4\n",
    "    return yaw,pitch,roll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Face_Detection_V(cap):\n",
    "    model_xml = r'models\\face-detection-adas-0001\\face-detection-adas-0001.xml'\n",
    "    model_bin = r'models\\face-detection-adas-0001\\face-detection-adas-0001.bin'\n",
    "    net = ie.read_network(model=model_xml, weights=model_bin)\n",
    "    exec_net = ie.load_network(network=net, device_name='CPU')\n",
    "    input_name = next(iter(net.input_info))\n",
    "    input_info = net.input_info[input_name]\n",
    "    n, c, h, w = input_info.tensor_desc.dims\n",
    "\n",
    "\n",
    "\n",
    "    while True:\n",
    "        # Read the next frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Resize the frame to match the input size of the model\n",
    "        resized_frame = cv2.resize(frame, (w, h))\n",
    "        input_data = resized_frame.transpose((2, 0, 1))\n",
    "\n",
    "        # Perform inference on the frame\n",
    "        output = exec_net.infer(inputs={input_name: input_data})\n",
    "\n",
    "        # Process the output data\n",
    "        output_name = next(iter(net.outputs))\n",
    "        output_data = output[output_name]\n",
    "        boxes = output_data[0][0]  # Assuming a single image was processed\n",
    "\n",
    "        # Loop through the detected faces and draw bounding boxes on the frame\n",
    "        Number_Of_Faces = 0\n",
    "        for box in boxes:\n",
    "            confidence = box[2]\n",
    "            if confidence > 0.5:  # Filter detections based on confidence threshold\n",
    "                Number_Of_Faces += 1\n",
    "                x_min = int(box[3] * frame.shape[1])\n",
    "                y_min = int(box[4] * frame.shape[0])\n",
    "                x_max = int(box[5] * frame.shape[1])\n",
    "                y_max = int(box[6] * frame.shape[0])\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "  \n",
    "\n",
    "        # Display the frame with bounding boxes\n",
    "        window_width = 800  # Set the desired window width\n",
    "        aspect_ratio = frame.shape[1] / frame.shape[0]\n",
    "        window_height = int(window_width / aspect_ratio)\n",
    "        frame = cv2.resize(frame, (window_width, window_height))\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        key = cv2.waitKey(1) \n",
    "        if  key ==27 or 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return boxes , Number_Of_Faces\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes , Number_Of_Faces = Face_Detection_V(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_points_v(video_path, boxes, Number_Of_Faces):\n",
    "    model_xml_landmarks = r'models\\facial-landmarks-35-adas-0002\\facial-landmarks-35-adas-0002.xml'\n",
    "    model_bin_landmarks = r'models\\facial-landmarks-35-adas-0002\\facial-landmarks-35-adas-0002.bin'\n",
    "    net_landmarks = ie.read_network(model=model_xml_landmarks, weights=model_bin_landmarks)\n",
    "    exec_net_landmarks = ie.load_network(network=net_landmarks, device_name='CPU')\n",
    "\n",
    "    # Prepare input data for the facial landmarks detection model\n",
    "    input_name_landmarks = next(iter(net_landmarks.input_info))\n",
    "    input_info_landmarks = net_landmarks.input_info[input_name_landmarks]\n",
    "    n_landmarks, c_landmarks, h_landmarks, w_landmarks = input_info_landmarks.tensor_desc.dims\n",
    "\n",
    "    video = cv2.VideoCapture(video_path)\n",
    "    captured_frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = video.read()\n",
    "\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_with_landmarks = frame.copy()\n",
    "\n",
    "        for i in range(Number_Of_Faces):\n",
    "            face_box = boxes[i]\n",
    "            x_min = int(face_box[3] * frame.shape[1])\n",
    "            y_min = int(face_box[4] * frame.shape[0])\n",
    "            x_max = int(face_box[5] * frame.shape[1])\n",
    "            y_max = int(face_box[6] * frame.shape[0])\n",
    "\n",
    "            face_image = frame[y_min:y_max, x_min:x_max]            \n",
    "            face_resized = cv2.resize(face_image, (w_landmarks, h_landmarks))\n",
    "            input_data_landmarks = face_resized.transpose((2, 0, 1))\n",
    "            output_landmarks = exec_net_landmarks.infer(inputs={input_name_landmarks: input_data_landmarks})\n",
    "\n",
    "            # Process the output of the facial landmarks detection model\n",
    "            output_name_landmarks = next(iter(net_landmarks.outputs))\n",
    "            landmarks = output_landmarks[output_name_landmarks]\n",
    "\n",
    "            # Reshape the landmarks into pairs of x and y coordinates\n",
    "            landmarks_xy = landmarks.reshape(-1, 2)\n",
    "               \n",
    "            \n",
    "            count = 0\n",
    "    \n",
    "            mark = []\n",
    " \n",
    "            for landmark in landmarks_xy:\n",
    "                \n",
    "\n",
    "                x, y = landmark\n",
    "\n",
    "                # Apply scaling factor to make the landmarks more visible\n",
    "                x = int(x * (x_max - x_min) + x_min)\n",
    "                y = int(y * (y_max - y_min) + y_min)\n",
    "                if count<4:\n",
    "                     mark.append(x)\n",
    "                     mark.append(y)\n",
    "                     count+=1\n",
    "\n",
    "                # Draw a circle at each landmark point\n",
    "            \n",
    "                cv2.circle(frame_with_landmarks, (x, y), radius=2, color=(0, 0, 255), thickness=1)\n",
    "      \n",
    "        # Add the frame with scattered landmarks to the captured frames list\n",
    "        captured_frames.append(frame_with_landmarks)\n",
    "        window_width = 800  # Set the desired window width\n",
    "        aspect_ratio = frame.shape[1] / frame.shape[0]\n",
    "        window_height = int(window_width / aspect_ratio)\n",
    "        frame_with_landmarks = cv2.resize(frame_with_landmarks, (window_width, window_height))\n",
    "        cv2.imshow('Video', frame_with_landmarks)\n",
    "        key = cv2.waitKey(1) \n",
    "        if  key ==27 or 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    # Release the video capture and close the window\n",
    "    video.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    return mark \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "land_mark = show_points_v(video_path,boxes,Number_Of_Faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def left_eye(image, First, Second):\n",
    "    # Determine the minimum and maximum x-coordinates of the eye region\n",
    "    x_max, x_min = max(First[0], First[1]), min(First[0], First[1])\n",
    "    \n",
    "    # Determine the minimum and maximum y-coordinates of the eye region\n",
    "    y_max, y_min = max(Second[0], Second[1]), min(Second[0], Second[1])\n",
    "    \n",
    "    # Define fixed offsets for the eye region\n",
    "    x_offset = 30\n",
    "    y_offset = 30\n",
    "    \n",
    "    # Adjust the coordinates using the fixed offsets\n",
    "    x_min -= x_offset\n",
    "    x_max += x_offset\n",
    "    y_min -= y_offset\n",
    "    y_max += y_offset\n",
    "    \n",
    "\n",
    "    \n",
    "    # Extract the eye region from the image\n",
    "    eye_region = image[y_min-50:y_max, x_min:x_max]\n",
    "    \n",
    "    return eye_region\n",
    "\n",
    "def right_eye(image, Third, Fourth):\n",
    "    # Determine the minimum and maximum x-coordinates of the eye region\n",
    "    x_max, x_min = max(Third[0], Third[1]), min(Third[0], Third[1])\n",
    "    \n",
    "    # Determine the minimum and maximum y-coordinates of the eye region\n",
    "    y_max, y_min = max(Fourth[0], Fourth[1]), min(Fourth[0], Fourth[1])\n",
    "    \n",
    "    # Define fixed offsets for the eye region\n",
    "    x_offset = 30\n",
    "    y_offset = 30\n",
    "    \n",
    "    # Adjust the coordinates using the fixed offsets\n",
    "    x_min -= x_offset\n",
    "    x_max += x_offset\n",
    "    y_min -= y_offset\n",
    "    y_max += y_offset\n",
    "    \n",
    "    \n",
    "    # Extract the eye region from the image\n",
    "    eye_region = image[y_min:y_max, x_min:x_max]\n",
    "    \n",
    "    return eye_region\n",
    "\n",
    "\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg as FigureCanvas\n",
    "\n",
    "def axes_to_array(ax):\n",
    "    # Get the figure associated with the axes\n",
    "    fig = ax.figure\n",
    "\n",
    "    # Create a new canvas\n",
    "    canvas = FigureCanvas(fig)\n",
    "\n",
    "    # Render the figure on the canvas\n",
    "    canvas.draw()\n",
    "\n",
    "    # Get the RGBA buffer from the canvas\n",
    "    buffer = canvas.buffer_rgba()\n",
    "\n",
    "    # Convert the buffer to a NumPy array\n",
    "    array = np.asarray(buffer)\n",
    "\n",
    "    return array\n",
    "\n",
    "def make_arrow(image,start_point, vector):\n",
    "\n",
    "    end_point = (start_point[0] + vector[0], start_point[1] + vector[1])\n",
    "\n",
    "    # Calculate the normalized vector in relation to the starting point\n",
    "    diff_x = end_point[0] - start_point[0]\n",
    "    diff_y = end_point[1] - start_point[1]\n",
    "    magnitude = np.linalg.norm([diff_x, diff_y])\n",
    "    normalized_vector = [diff_x / magnitude, diff_y / magnitude]\n",
    "\n",
    "    # Scale down the vector to ensure visibility\n",
    "    scale_factor = 10 # Adjust this factor as needed\n",
    "    scaled_vector = [normalized_vector[0] * scale_factor, normalized_vector[1] * scale_factor]\n",
    "  \n",
    "    # Convert the image to RGB if it's in BGR format\n",
    "    if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Create a figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plot the image\n",
    "    ax.imshow(image)\n",
    "    ax.axis('off')\n",
    "    ax.arrow(start_point[0], start_point[1],\n",
    "             scaled_vector[0], scaled_vector[1],\n",
    "             head_width=18, head_length=100,\n",
    "             fc='red', ec='red')\n",
    "\n",
    "    # Convert the buffer to a NumPy array\n",
    "    array = axes_to_array(ax)\n",
    "    array = cv2.cvtColor(array, cv2.COLOR_BGR2RGB)\n",
    "    return array\n",
    "    \n",
    "def make_gaze(image,right_eye,left_eye,yaw, pitch, roll,eye_center_left,eye_center_right):\n",
    "    gaze_xml = r'models\\gaze-estimation-adas-0002\\gaze-estimation-adas-0002.xml'\n",
    "    gaze_bin = r'models\\gaze-estimation-adas-0002\\gaze-estimation-adas-0002.bin'\n",
    "    net_gaze = ie.read_network(model=gaze_xml, weights=gaze_bin)\n",
    "    ext_gaze = ie.load_network(network=net_gaze, device_name='CPU')\n",
    "    input_name = next(iter(net_gaze.input_info))\n",
    "    input_info = net_gaze.input_info[input_name]\n",
    "\n",
    "    right_eye_image_resized = cv2.resize(right_eye, (60, 60))\n",
    "    right_eye_input = right_eye_image_resized.transpose((2, 0, 1)) \n",
    "\n",
    "    # Perform gaze estimation for the left eye\n",
    "    left_eye_image_resized = cv2.resize(left_eye, (60, 60))\n",
    "    left_eye_input = left_eye_image_resized.transpose((2, 0, 1))  # Transpose to B, C, H, W format\n",
    "    output_left = ext_gaze.infer(inputs={'right_eye_image': right_eye_input,'left_eye_image': left_eye_input, 'head_pose_angles': [yaw, pitch, roll]})\n",
    "    gaze_vector = output_left['gaze_vector'].flatten()\n",
    "    left_eye_center = eye_center_left\n",
    "    right_eye_center = eye_center_right\n",
    "\n",
    "    combined_center =  ((x + y) / 2 for x, y in zip(left_eye_center, right_eye_center))\n",
    "    return  tuple(combined_center),gaze_vector[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gaze_arrow(cap2, land_mark):\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap2.read()\n",
    "        if not ret:\n",
    "           break\n",
    "        yaw, pitch, roll = head_pose_angles(frame)\n",
    "        first = land_mark[:3:2]\n",
    "        second = land_mark[1:4:2]\n",
    "        third = land_mark[4:7:2]\n",
    "        fourth = land_mark[5::2]\n",
    "        eye_center_left = (int((first[0] + first[1]) / 2), int((second[0] + second[1]) / 2))\n",
    "        eye_center_right = (int((third[0] + third[1]) / 2), int((fourth[0] + fourth[1]) / 2))\n",
    "        left_eye1 = left_eye(frame, first, second)\n",
    "        right_eye1 = right_eye(frame, third, fourth)\n",
    "\n",
    "        start_point  , vector = make_gaze(frame, right_eye1, left_eye1, yaw, pitch, roll, eye_center_left, eye_center_right)\n",
    "        \n",
    "\n",
    "        frame2 = make_arrow(frame,start_point,vector)\n",
    "\n",
    "\n",
    "        window_width = 800  # Set the desired window width\n",
    "        aspect_ratio = frame.shape[1] / frame.shape[0]\n",
    "        window_height = int(window_width / aspect_ratio)\n",
    "\n",
    "        frame2 = cv2.resize(frame2, (window_width, window_height))\n",
    "\n",
    "        # Apply color transformation to frame2 if desired\n",
    "     \n",
    "      \n",
    "        cv2.imshow('Video', frame2)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Face_Direction_V(cap):\n",
    "    model_xml = r'models\\face-detection-adas-0001\\face-detection-adas-0001.xml'\n",
    "    model_bin = r'models\\face-detection-adas-0001\\face-detection-adas-0001.bin'\n",
    "    net = ie.read_network(model=model_xml, weights=model_bin)\n",
    "    exec_net = ie.load_network(network=net, device_name='CPU')\n",
    "    input_name = next(iter(net.input_info))\n",
    "    input_info = net.input_info[input_name]\n",
    "    n, c, h, w = input_info.tensor_desc.dims\n",
    "\n",
    "\n",
    "\n",
    "    while True:\n",
    "        # Read the next frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Resize the frame to match the input size of the model\n",
    "        resized_frame = cv2.resize(frame, (w, h))\n",
    "        input_data = resized_frame.transpose((2, 0, 1))\n",
    "\n",
    "        # Perform inference on the frame\n",
    "        output = exec_net.infer(inputs={input_name: input_data})\n",
    "\n",
    "        # Process the output data\n",
    "        output_name = next(iter(net.outputs))\n",
    "        output_data = output[output_name]\n",
    "        boxes = output_data[0][0]  # Assuming a single image was processed\n",
    "\n",
    "        # Loop through the detected faces and draw bounding boxes on the frame\n",
    "        Number_Of_Faces = 0\n",
    "        for box in boxes:\n",
    "            confidence = box[2]\n",
    "            if confidence > 0.5:  # Filter detections based on confidence threshold\n",
    "                Number_Of_Faces += 1\n",
    "                x_min = int(box[3] * frame.shape[1])\n",
    "                y_min = int(box[4] * frame.shape[0])\n",
    "                x_max = int(box[5] * frame.shape[1])\n",
    "                y_max = int(box[6] * frame.shape[0])\n",
    "                cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
    "                first = land_mark[:3:2]\n",
    "                second = land_mark[1:4:2]\n",
    "                third = land_mark[4:7:2]\n",
    "                fourth = land_mark[5::2]\n",
    "                yaw, pitch, roll = head_pose_angles(frame)\n",
    "                right_eye_image,left_eye_image = right_eye(frame,third,fourth),left_eye(frame,first,second)\n",
    "     \n",
    "                eye_center_left = (int((first[0] + first[1]) / 2), int((second[0] + second[1]) / 2))\n",
    "                eye_center_right = (int((third[0] + third[1]) / 2), int((fourth[0] + fourth[1]) / 2))\n",
    "                gaze_center, gaze_vector = make_gaze(frame, right_eye_image, left_eye_image, yaw, pitch, roll, eye_center_left, eye_center_right)\n",
    "           \n",
    "                gaze_direction = head_pose_direction(gaze_vector)\n",
    "         \n",
    "                cv2.putText(frame, gaze_direction, (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "        # Display the frame with bounding boxes\n",
    "        window_width = 800  # Set the desired window width\n",
    "        aspect_ratio = frame.shape[1] / frame.shape[0]\n",
    "        window_height = int(window_width / aspect_ratio)\n",
    "        frame = cv2.resize(frame, (window_width, window_height))\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "        key = cv2.waitKey(1) \n",
    "        if  key ==27 or 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vector(vector):\n",
    "    norm = np.linalg.norm(vector)\n",
    "    if norm == 0:\n",
    "        return vector\n",
    "    return vector / norm\n",
    "\n",
    "def head_pose_direction(gaze_vector):\n",
    "    normalized_vector = normalize_vector(gaze_vector)\n",
    "    if normalized_vector[0] > 0.5:\n",
    "        if normalized_vector[1] > 0.3:\n",
    "            direction = 'Up-left'\n",
    "        elif normalized_vector[1] < -0.3:\n",
    "            direction = 'Down-left'\n",
    "        else:\n",
    "            direction = 'left'\n",
    "    elif normalized_vector[0] < -0.3:\n",
    "        if normalized_vector[1] > 0.3:\n",
    "            direction = 'Up-right'\n",
    "        elif normalized_vector[1] < -0.3:\n",
    "            direction = 'Down-right'\n",
    "        else:\n",
    "            direction = 'right'\n",
    "    elif normalized_vector[1] > 0.3:\n",
    "        direction = 'Up'\n",
    "    elif normalized_vector[1] < -0.3:\n",
    "        direction = 'Down'\n",
    "    else:\n",
    "        direction = 'Straight'\n",
    "\n",
    "    return direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_path)\n",
    "Face_Direction_V(cap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = r'C:\\Users\\91702\\OneDrive\\Desktop\\OpenVIno\\data_videos\\pexels.avi'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "a = make_gaze_arrow(cap,land_mark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openvino_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
